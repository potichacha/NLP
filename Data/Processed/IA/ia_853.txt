I have to work on an AI project. From now on, I will explain the project I have to do. The data is divided into three parts. 1. Training data: train.csv 2. Test data: test.csv 3. Submission file: sample_submission.csv The train.csv file consists of 3 columns of id, text and label, and the test.csv file consists of 2 columns of id and text. The sample_submission.csv file consists of id and label columns. Label consists of a total of 8, and the text column consists of news article data. Write code to train and infer a model that classifies it as a catboost model. Don't do train_test_split. Set catboost's parameters : iterations to 100, learning_rate to 0.5, loss_function to 'MultiClass', thread_count to -1, random_state to 42. When indexing X data, you must index text column only. And you should make dataframe using pandas column indexing. When making the Pool, you must specify text_features argument, the list of column names of texts. You should read sample_submission.csv with pandas and flatten prediction results for numpy method. Give me a code for above description.

Sure, here's a code that should accomplish the task you described: Note that in this code, we are using the  Pool  object from CatBoost to handle the data. The  text_features  argument is used to indicate that we want to use the  text  column for text classification. Also, since we are not doing any kind of training/testing split, we are using all of the training data to train the model. Finally, we are using the  predict  method of the CatBoostClassifier to make predictions on the test data, and then flattening the predictions to match the format of the submission file.

Use hyperopt, 3-StratifiedKFold, macro-f1-score to optimize model above. Optimize parameters learning_rate(0.1 to 1), depth(3 to 8), l2_leaf_reg. And you should set 42 to random seed, use default_rng of numpy to set seed of hyperopt. Use uniform or quniform distribution for search space. And you should use GPU for training catboost model. When training the final model, use 1/5 times of best learning_rate and iterations 1000. Finally, you should make submission.csv.

Sure, here's an updated code that uses hyperopt for hyperparameter tuning, 3-StratifiedKFold for cross-validation, and macro-f1-score for evaluation. It also sets up GPU training for CatBoost, and trains the final model using a learning_rate that is 1/5 times the best learning_rate found during hyperparameter tuning.

Finialize the above code. Train the best_model and make the prediction for test data. Give me an additional code.

Sure! Here's the final code that trains the best model found by hyperopt, makes the prediction for the test data, and creates the submission file: